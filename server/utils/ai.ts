// AI Search utilities using Transformers.js
// Using WASM backend for better Bun compatibility
import { resolve } from 'path'

// å¼ºåˆ¶ç¦ç”¨ onnxruntime-nodeï¼Œä½¿ç”¨ WASM åç«¯
// å¿…é¡»åœ¨å¯¼å…¥ @xenova/transformers ä¹‹å‰è®¾ç½®
process.env.ONNX_DISABLE_NODE = '1'

// Models
const TEXT_MODEL = 'Xenova/all-MiniLM-L6-v2'
const VISION_MODEL = 'Xenova/clip-vit-base-patch32'

// æ¨¡å‹ç¼“å­˜è·¯å¾„ - ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .models æ–‡ä»¶å¤¹ï¼Œé¿å…è¢« node_modules æ“ä½œæ¸…é™¤
const MODEL_CACHE_PATH = resolve(process.cwd(), '.models')

let textEmbedder: any = null
let visionEmbedder: any = null
let isInitialized = false
let initError: Error | null = null

async function getEmbedder(type: 'text' | 'vision' = 'text') {
    if (initError) throw initError

    // Check specific embedder
    if (type === 'text' && textEmbedder) return textEmbedder
    if (type === 'vision' && visionEmbedder) return visionEmbedder

    try {
        // Dynamic import to avoid SSR issues
        const { pipeline, env } = await import('@xenova/transformers')

        // Force WASM backend to avoid onnxruntime-node issues
        env.backends.onnx.wasm.numThreads = 1
        // ç¦ç”¨ webgpu å’Œå…¶ä»–åç«¯ï¼Œåªä½¿ç”¨ wasm
        if (env.backends.onnx.webgpu) {
            env.backends.onnx.webgpu.disabled = true
        }
        env.allowLocalModels = true
        env.localModelPath = MODEL_CACHE_PATH
        env.cacheDir = MODEL_CACHE_PATH
        env.useBrowserCache = false
        // Use domestic mirror for faster downloads in China
        env.remoteHost = 'https://hf-mirror.com/'
        env.remotePathTemplate = '{model}/resolve/{revision}/'

        console.log(`ğŸ¤– Initializing AI ${type} model (this may take a moment)...`)
        console.log(`ğŸŒ Model Source: ${env.remoteHost}`)

        if (type === 'text') {
            textEmbedder = await pipeline('feature-extraction', TEXT_MODEL, {
                quantized: true,
                progress_callback: (progress: any) => {
                    if (progress.status === 'progress') {
                        console.log(`ğŸ“¥ Downloading Text Model: ${Math.round(progress.progress)}%`)
                    }
                }
            })
            return textEmbedder
        }

        if (type === 'vision') {
            visionEmbedder = await pipeline('image-feature-extraction', VISION_MODEL, {
                quantized: true,
                progress_callback: (progress: any) => {
                    if (progress.status === 'progress') {
                        console.log(`ğŸ“¥ Downloading Vision Model: ${Math.round(progress.progress)}%`)
                    }
                }
            })
            return visionEmbedder
        }

        isInitialized = true
        console.log('âœ… AI models initialized successfully')
    } catch (error) {
        initError = error as Error
        console.error('âŒ Failed to initialize AI model:', error)
        throw error
    }
}

/**
 * Generate embedding vector for text
 */
export async function getTextEmbedding(text: string): Promise<number[]> {
    try {
        const embedder = await getEmbedder('text')
        const output = await embedder(text, { pooling: 'mean', normalize: true })
        return Array.from(output.data)
    } catch (error) {
        console.error('Failed to generate text embedding:', error)
        throw error
    }
}

/**
 * Generate embedding vector for image
 * @param imageInput - Buffer or URL of the image
 */
export async function getImageEmbedding(imageInput: Buffer | string): Promise<number[]> {
    try {
        const embedder = await getEmbedder('vision')

        // Import RawImage for proper image handling
        const { RawImage } = await import('@xenova/transformers')

        let image
        if (Buffer.isBuffer(imageInput)) {
            // Convert Buffer to RawImage using Uint8Array
            const uint8Array = new Uint8Array(imageInput)
            image = await RawImage.fromBlob(new Blob([uint8Array]))
        } else {
            // Assume it's a URL or path
            image = await RawImage.read(imageInput)
        }

        // Run inference
        const output = await embedder(image, { pooling: 'mean', normalize: true })
        return Array.from(output.data)
    } catch (error) {
        console.error('Failed to generate image embedding:', error)
        throw error
    }
}

/**
 * Generate tags for an image using Zero-Shot Classification
 * Uses the provided image embedding to find best matching labels
 */
// Pre-defined Candidate Labels for Lost & Found (æ ¡å›­å¸¸è§ç‰©å“)
const CANDIDATE_LABELS = [
    // ç”µå­è®¾å¤‡
    'phone', 'laptop', 'tablet', 'headphones', 'earbuds', 'charger', 'usb drive', 'power bank', 'mouse', 'keyboard',
    // è¯ä»¶å¡ç‰‡
    'wallet', 'credit card', 'id card', 'student card', 'passport', 'bus card',
    // é’¥åŒ™
    'keys', 'key chain', 'access card',
    // åŒ…ç±»
    'backpack', 'bag', 'handbag', 'pencil case', 'luggage',
    // ä¹¦ç±æ–‡å…·
    'book', 'notebook', 'textbook', 'pen', 'pencil', 'calculator',
    // è¡£ç‰©é…é¥°
    'jacket', 'coat', 'hat', 'cap', 'scarf', 'gloves', 'shoes', 'glasses', 'sunglasses', 'watch', 'jewelry', 'ring', 'necklace', 'bracelet',
    // ç”Ÿæ´»ç”¨å“
    'umbrella', 'water bottle', 'thermos', 'lunch box', 'cup', 'mug',
    // è¿åŠ¨å™¨æ
    'basketball', 'football', 'tennis racket', 'badminton racket', 'sports shoes',
    // é¢œè‰²æè¿°
    'black object', 'white object', 'blue object', 'red object', 'green object', 'pink object', 'yellow object'
]

// è‹±æ–‡æ ‡ç­¾åˆ°ä¸­æ–‡çš„æ˜ å°„
const LABEL_TO_CHINESE: Record<string, string> = {
    // ç”µå­è®¾å¤‡
    'phone': 'æ‰‹æœº', 'laptop': 'ç¬”è®°æœ¬ç”µè„‘', 'tablet': 'å¹³æ¿', 'headphones': 'è€³æœº', 'earbuds': 'è€³å¡',
    'charger': 'å……ç”µå™¨', 'usb drive': 'Uç›˜', 'power bank': 'å……ç”µå®', 'mouse': 'é¼ æ ‡', 'keyboard': 'é”®ç›˜',
    // è¯ä»¶å¡ç‰‡
    'wallet': 'é’±åŒ…', 'credit card': 'é“¶è¡Œå¡', 'id card': 'èº«ä»½è¯', 'student card': 'å­¦ç”Ÿè¯',
    'passport': 'æŠ¤ç…§', 'bus card': 'å…¬äº¤å¡',
    // é’¥åŒ™
    'keys': 'é’¥åŒ™', 'key chain': 'é’¥åŒ™é“¾', 'access card': 'é—¨ç¦å¡',
    // åŒ…ç±»
    'backpack': 'åŒè‚©åŒ…', 'bag': 'åŒ…', 'handbag': 'æ‰‹æåŒ…', 'pencil case': 'ç¬”è¢‹', 'luggage': 'è¡Œæç®±',
    // ä¹¦ç±æ–‡å…·
    'book': 'ä¹¦', 'notebook': 'ç¬”è®°æœ¬', 'textbook': 'æ•™æ', 'pen': 'ç¬”', 'pencil': 'é“…ç¬”', 'calculator': 'è®¡ç®—å™¨',
    // è¡£ç‰©é…é¥°
    'jacket': 'å¤¹å…‹', 'coat': 'å¤–å¥—', 'hat': 'å¸½å­', 'cap': 'é¸­èˆŒå¸½', 'scarf': 'å›´å·¾', 'gloves': 'æ‰‹å¥—',
    'shoes': 'é‹å­', 'glasses': 'çœ¼é•œ', 'sunglasses': 'å¢¨é•œ', 'watch': 'æ‰‹è¡¨', 'jewelry': 'é¦–é¥°',
    'ring': 'æˆ’æŒ‡', 'necklace': 'é¡¹é“¾', 'bracelet': 'æ‰‹é“¾',
    // ç”Ÿæ´»ç”¨å“
    'umbrella': 'é›¨ä¼', 'water bottle': 'æ°´æ¯', 'thermos': 'ä¿æ¸©æ¯', 'lunch box': 'é¥­ç›’', 'cup': 'æ¯å­', 'mug': 'é©¬å…‹æ¯',
    // è¿åŠ¨å™¨æ
    'basketball': 'ç¯®çƒ', 'football': 'è¶³çƒ', 'tennis racket': 'ç½‘çƒæ‹', 'badminton racket': 'ç¾½æ¯›çƒæ‹', 'sports shoes': 'è¿åŠ¨é‹',
    // é¢œè‰²
    'black': 'é»‘è‰²', 'white': 'ç™½è‰²', 'blue': 'è“è‰²', 'red': 'çº¢è‰²', 'green': 'ç»¿è‰²', 'pink': 'ç²‰è‰²', 'yellow': 'é»„è‰²'
}

// Zero-shot classifier instance
let zeroShotClassifier: any = null

export async function generateImageTags(imageBuffer: Buffer): Promise<string[]> {
    try {
        // Initialize zero-shot classifier if needed
        if (!zeroShotClassifier) {
            console.log('ğŸ·ï¸ Initializing zero-shot image classifier...')
            const { pipeline, env } = await import('@xenova/transformers')

            env.backends.onnx.wasm.numThreads = 1
            env.allowLocalModels = true
            env.localModelPath = MODEL_CACHE_PATH
            env.cacheDir = MODEL_CACHE_PATH
            env.useBrowserCache = false
            env.remoteHost = 'https://hf-mirror.com/'
            env.remotePathTemplate = '{model}/resolve/{revision}/'

            zeroShotClassifier = await pipeline('zero-shot-image-classification', VISION_MODEL, {
                quantized: true,
                progress_callback: (progress: any) => {
                    if (progress.status === 'progress') {
                        console.log(`ğŸ“¥ Downloading Classifier: ${Math.round(progress.progress)}%`)
                    }
                }
            })
            console.log('âœ… Zero-shot classifier ready')
        }

        // Convert Buffer to Blob for the classifier
        const { RawImage } = await import('@xenova/transformers')
        const uint8Array = new Uint8Array(imageBuffer)
        const image = await RawImage.fromBlob(new Blob([uint8Array]))

        // Run zero-shot classification
        const results = await zeroShotClassifier(image, CANDIDATE_LABELS)

        console.log('ğŸ” Zero-shot results:', results.slice(0, 5).map((r: any) => `${r.label} (${r.score.toFixed(2)})`).join(', '))

        // Return top labels with score > 0.1, translated to Chinese
        const topTags = results
            .filter((r: any) => r.score > 0.1)
            .slice(0, 5)
            .map((r: any) => {
                const label = r.label.replace(' object', '')
                // ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œå¦‚æœæ²¡æœ‰æ˜ å°„åˆ™è¿”å›åŸæ–‡
                return LABEL_TO_CHINESE[label] || label
            })

        return topTags
    } catch (error) {
        console.error('Failed to generate tags:', error)
        return []
    }
}

/**
 * Calculate cosine similarity between two vectors
 */
export function cosineSimilarity(a: number[], b: number[]): number {
    if (a.length !== b.length) return 0

    let dotProduct = 0
    let normA = 0
    let normB = 0

    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i]
        normA += a[i] * a[i]
        normB += b[i] * b[i]
    }

    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB))
}

/**
 * Find similar posts using semantic search (Text)
 */
export async function findSimilarPosts(
    queryText: string,
    posts: { id: number; title: string; content: string }[],
    topK: number = 10,
    threshold: number = 0.3
): Promise<{ id: number; score: number }[]> {
    try {
        const queryEmbedding = await getTextEmbedding(queryText)

        const results: { id: number; score: number }[] = []

        for (const post of posts) {
            const postText = `${post.title} ${post.content}`
            const postEmbedding = await getTextEmbedding(postText)
            const score = cosineSimilarity(queryEmbedding, postEmbedding)

            if (score >= threshold) {
                results.push({ id: post.id, score })
            }
        }

        // Sort by score descending and take top K
        return results.sort((a, b) => b.score - a.score).slice(0, topK)
    } catch (error) {
        console.error('Error in findSimilarPosts:', error)
        // Fallback: return posts sorted by ID if AI fails
        return posts.slice(0, topK).map((p, i) => ({
            id: p.id,
            score: 1 - (i * 0.1) // Fake descending scores
        }))
    }
}


